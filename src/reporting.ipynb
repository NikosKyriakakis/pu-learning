{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from console import *\n",
    "\n",
    "\n",
    "def plot_results(experiment, metric, n_iters=3):\n",
    "    itrs = []\n",
    "    test_loss = []\n",
    "    negative_cls_metric = []\n",
    "    positive_cls_metric = []\n",
    "    weighted_avg_metric = []\n",
    "    macro_avg_metric = []\n",
    "\n",
    "    try:\n",
    "        for i in range(n_iters):\n",
    "            itr = experiment[f\"iteration-{i}\"]\n",
    "            itrs.append(f\"iteration-{i}\")\n",
    "\n",
    "            if metric != \"test_loss\":\n",
    "                negative_cls_metric.append(itr[\"0\"][metric])\n",
    "                positive_cls_metric.append(itr[\"1\"][metric])\n",
    "                weighted_avg_metric.append(itr[\"weighted avg\"][metric])\n",
    "                macro_avg_metric.append(itr[\"macro avg\"][metric])\n",
    "            else:\n",
    "                test_loss.append(itr[\"test_loss\"])\n",
    "\n",
    "        if n_iters == 1:\n",
    "            width = 0.1\n",
    "        else:\n",
    "            width = 0.5\n",
    "\n",
    "        if metric != \"test_loss\":\n",
    "            fig, axs = plt.subplots(2, 2)\n",
    "            fig.suptitle(f\"Measurements for {experiment['name']}\")\n",
    "            fig.set_figheight(10)\n",
    "            fig.set_figwidth(10)\n",
    "\n",
    "            axs[0][0].set_title(f\"Negative class {metric}\")\n",
    "            p = axs[0][0].bar(itrs, negative_cls_metric, width=width, color=\"tab:blue\")\n",
    "            axs[0][0].bar_label(p, label_type=\"edge\")\n",
    "\n",
    "            axs[0][1].set_title(f\"Positive class {metric}\")\n",
    "            p = axs[0][1].bar(itrs, positive_cls_metric, width=width, color=\"tab:purple\")\n",
    "            axs[0][1].bar_label(p, label_type=\"edge\")\n",
    "\n",
    "            axs[1][0].set_title(f\"Weighted avg {metric}\")\n",
    "            p = axs[1][0].bar(itrs, weighted_avg_metric, width=width, color=\"tab:orange\")\n",
    "            axs[1][0].bar_label(p, label_type=\"edge\")\n",
    "\n",
    "            axs[1][1].set_title(f\"Macro avg {metric}\")\n",
    "            p = axs[1][1].bar(itrs, macro_avg_metric, width=width, color=\"tab:green\")\n",
    "            axs[1][1].bar_label(p, label_type=\"edge\")\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.suptitle(f\"Measurements for {experiment['name']}\")\n",
    "            plt.bar(itrs, test_loss, width=width, color=\"tab:blue\")\n",
    "            plt.show()\n",
    "    except KeyError as key_err:\n",
    "        if \"iteration\" not in str(key_err): \n",
    "            print(error(f\"Invalid metric key provided --> {key_err}\"))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_json = {}\n",
    "with open(\"report.json\", 'r') as fp:\n",
    "    report_json = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_keys = report_json.keys()\n",
    "\n",
    "dim300 = []\n",
    "dim200 = []\n",
    "dim100 = []\n",
    "dim50  = []\n",
    "\n",
    "for key in exp_keys:\n",
    "    if report_json[key][\"dim\"] == 300:\n",
    "        dim300.append(key)\n",
    "    elif report_json[key][\"dim\"] == 200:\n",
    "        dim200.append(key)\n",
    "    elif report_json[key][\"dim\"] == 100:\n",
    "        dim100.append(key)\n",
    "    else:\n",
    "        dim50.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-Score & 300-dim GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in dim300:\n",
    "    exp = report_json[exp_key]\n",
    "    n_iters = 0\n",
    "    for k in exp.keys():\n",
    "        if \"iteration\" in k:\n",
    "            n_iters += 1\n",
    "    plot_results(experiment=exp, metric=\"f1-score\", n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in dim300:\n",
    "    exp = report_json[exp_key]\n",
    "    n_iters = 0\n",
    "    for k in exp.keys():\n",
    "        if \"iteration\" in k:\n",
    "            n_iters += 1\n",
    "    plot_results(experiment=exp, metric=\"recall\", n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in dim300:\n",
    "    exp = report_json[exp_key]\n",
    "    n_iters = 0\n",
    "    for k in exp.keys():\n",
    "        if \"iteration\" in k:\n",
    "            n_iters += 1\n",
    "    plot_results(experiment=exp, metric=\"precision\", n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in dim300:\n",
    "    exp = report_json[exp_key]\n",
    "    n_iters = 0\n",
    "    for k in exp.keys():\n",
    "        if \"iteration\" in k:\n",
    "            n_iters += 1\n",
    "    plot_results(experiment=exp, metric=\"test_loss\", n_iters=n_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
